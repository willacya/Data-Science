{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Titanic: Machine Learning from Disaster",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "* Description:\n    The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n* Problem definition: In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport pandas as pd\nimport sklearn.linear_model as lm\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\n%matplotlib inline",
   "execution_count": 1,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Data Exploration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')",
   "execution_count": 2,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print (\"Dimension of train data {}\".format(train.shape))\nprint (\"Dimension of test data {}\".format(test.shape))",
   "execution_count": 9,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print (\"Basic statistical description:\")\ntrain.describe()",
   "execution_count": 10,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Comparison between test and train data",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### From following cells, we could know that train and test data are split by PassengerId.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train.tail()",
   "execution_count": 11,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "test.head()",
   "execution_count": 12,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Let's look at data graphically. We could see that all the distribution of features are similar.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "plt.rc('font', size=13)\nfig = plt.figure(figsize=(18, 8))\nalpha = 0.6\n\nax1 = plt.subplot2grid((2,3), (0,0))\ntrain.Age.value_counts().plot(kind='kde', color='#FA2379', label='train', alpha=alpha)\ntest.Age.value_counts().plot(kind='kde', label='test', alpha=alpha)\nax1.set_xlabel('Age')\nax1.set_title(\"What's the distribution of age?\" )\nplt.legend(loc='best')\n\nax2 = plt.subplot2grid((2,3), (0,1))\ntrain.Pclass.value_counts().plot(kind='barh', color='#FA2379', label='train', alpha=alpha)\ntest.Pclass.value_counts().plot(kind='barh', label='test', alpha=alpha)\nax2.set_ylabel('Pclass')\nax2.set_xlabel('Frequency')\nax2.set_title(\"What's the distribution of Pclass?\" )\nplt.legend(loc='best')\n\nax3 = plt.subplot2grid((2,3), (0,2))\ntrain.Sex.value_counts().plot(kind='barh', color='#FA2379', label='train', alpha=alpha)\ntest.Sex.value_counts().plot(kind='barh', label='test', alpha=alpha)\nax3.set_ylabel('Sex')\nax3.set_xlabel('Frequency')\nax3.set_title(\"What's the distribution of Sex?\" )\nplt.legend(loc='best')\n\nax4 = plt.subplot2grid((2,3), (1,0), colspan=2)\ntrain.Fare.value_counts().plot(kind='kde', color='#FA2379', label='train', alpha=alpha)\ntest.Fare.value_counts().plot(kind='kde', label='test', alpha=alpha)\nax4.set_xlabel('Fare')\nax4.set_title(\"What's the distribution of Fare?\" )\nplt.legend(loc='best')\n\nax5 = plt.subplot2grid((2,3), (1,2))\ntrain.Embarked.value_counts().plot(kind='barh', color='#FA2379', label='train', alpha=alpha)\ntest.Embarked.value_counts().plot(kind='barh', label='test', alpha=alpha)\nax5.set_ylabel('Embarked')\nax5.set_xlabel('Frequency')\nax5.set_title(\"What's the distribution of Embarked?\" )\nplt.legend(loc='best')\nplt.tight_layout()",
   "execution_count": 13,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### We could know that the numbers of survived and died people are close to balanced.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print (train.Survived.value_counts())",
   "execution_count": 15,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Look closely to the train data",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### We could see that the density of survived is higher than not survived under age 10.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "fig = plt.figure(figsize=(15, 6))\n\ntrain[train.Survived==0].Age.value_counts().plot(kind='density', color='#FA2379', label='Not Survived', alpha=alpha)\ntrain[train.Survived==1].Age.value_counts().plot(kind='density', label='Survived', alpha=alpha)\nplt.xlabel('Age')\nplt.title(\"What's the distribution of Age?\" )\nplt.legend(loc='best')\nplt.grid()",
   "execution_count": 16,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### We could observe that the survived rate of female is higher than male about 50%",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "df_male = train[train.Sex=='male'].Survived.value_counts().sort_index()\ndf_female = train[train.Sex=='female'].Survived.value_counts().sort_index()\nfig = plt.figure(figsize=(18, 6))\n\nax1 = plt.subplot2grid((1,2), (0,0))\ndf_female.plot(kind='barh', color='#FA2379', label='Female', alpha=alpha)\ndf_male.plot(kind='barh', label='Male', alpha=alpha-0.1)\nax1.set_xlabel('Frequrncy')\nax1.set_yticklabels(['Died', 'Survived'])\nax1.set_title(\"Who will survived with respect to sex?\" )\nplt.legend(loc='best')\nplt.grid()\n\nax2 = plt.subplot2grid((1,2), (0,1))\n(df_female/train[train.Sex=='female'].shape[0]).plot(kind='barh', color='#FA2379', label='Female', alpha=alpha)\n(df_male/train[train.Sex=='male'].shape[0]).plot(kind='barh', label='Male', alpha=alpha-0.1)\nax2.set_xlabel('Rate')\nax2.set_yticklabels(['Died', 'Survived'])\nax2.set_title(\"What's the survived rate with respect to sex?\" )\nplt.legend(loc='best')\nplt.grid()",
   "execution_count": 17,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### We could observed that the class of people affected the posibility of being survived.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "df_male = train[train.Sex=='male']\ndf_female = train[train.Sex=='female']\nfig = plt.figure(figsize=(18, 6))\n\nax1 = plt.subplot2grid((1,4), (0,0))\ndf_female[df_female.Pclass<3].Survived.value_counts().sort_index().plot(kind='bar', color='#FA2379', alpha=alpha)\nax1.set_ylabel('Frequrncy')\nax1.set_ylim((0,350))\nax1.set_xticklabels(['Died', 'Survived'])\nax1.set_title(\"How will high-class female survived?\", y=1.05)\nplt.grid()\n\nax2 = plt.subplot2grid((1,4), (0,1))\ndf_female[df_female.Pclass==3].Survived.value_counts().sort_index().plot(kind='bar', color='#23FA79', alpha=alpha)\nax2.set_ylabel('Frequrncy')\nax2.set_ylim((0,350))\nax2.set_xticklabels(['Died', 'Survived'])\nax2.set_title(\"How will low-class female survived?\", y=1.05)\nplt.grid()\n\nax3 = plt.subplot2grid((1,4), (0,2))\ndf_male[df_male.Pclass<3].Survived.value_counts().sort_index().plot(kind='bar', color='#00FA23', alpha=alpha)\nax3.set_ylabel('Frequrncy')\nax3.set_ylim((0,350))\nax3.set_xticklabels(['Died', 'Survived'])\nax3.set_title(\"How will high-class male survived?\", y=1.05)\nplt.grid()\n\nax4 = plt.subplot2grid((1,4), (0,3))\ndf_male[df_male.Pclass==3].Survived.value_counts().sort_index().plot(kind='bar', color='#2379FA', alpha=alpha)\nax4.set_ylabel('Frequrncy')\nax4.set_ylim((0,350))\nax4.set_xticklabels(['Died', 'Survived'])\nax4.set_title(\"How will low-class male survived?\", y=1.05)\nplt.grid()\nplt.tight_layout()",
   "execution_count": 18,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### We could see that if the value of Ticket is the same, passenger would be close, like friends or familes. But sometimes it will not be the case, so we need to carefully handle it.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train[train.Ticket=='1601']",
   "execution_count": 19,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train[train.Ticket=='CA 2144']",
   "execution_count": 20,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Data Cleaning",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Let's see how many missing values we have on each column. ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train.isnull().sum()",
   "execution_count": 21,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "test.isnull().sum()",
   "execution_count": 22,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Thanks for [this amazing sharing](https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic/notebook), now we have a good way to replace missing values with sensible values. Let's assume the embarked is related with fare and pclass.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Missing values on Embarked",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "fig = plt.figure(figsize=(8, 5))\nax = fig.add_subplot(111)\nax = train.boxplot(column='Fare', by=['Embarked','Pclass'], ax=ax)\nplt.axhline(y=80, color='green')\nax.set_title('', y=1.1)\n\ntrain[train.Embarked.isnull()][['Fare', 'Pclass', 'Embarked']]",
   "execution_count": 23,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### From the above boxplot, we should replace NA with C because most people who had Pclass 1 and Fare 80 would be Embarked C",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "_ = train.set_value(train.Embarked.isnull(), 'Embarked', 'C')",
   "execution_count": 24,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Missing values on Fare",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### By fixing the values of Embarked and Pclass, we could plot histogram of Fare. And we should use the most common value to replace the NA value of Fare.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "fig = plt.figure(figsize=(8, 5))\nax = fig.add_subplot(111)\ntest[(test.Pclass==3)&(test.Embarked=='S')].Fare.hist(bins=100, ax=ax)\ntest[test.Fare.isnull()][['Pclass', 'Fare', 'Embarked']]\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.title('Histogram of Fare, Plcass 3 and Embarked S')\n\ntest[test.Fare.isnull()][['Pclass', 'Fare', 'Embarked']]",
   "execution_count": 25,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print (\"The top 5 most common value of Fare\")\ntest[(test.Pclass==3)&(test.Embarked=='S')].Fare.value_counts().head()",
   "execution_count": 27,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "_ = test.set_value(test.Fare.isnull(), 'Fare', 8.05)",
   "execution_count": 28,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Replace the missing value of Cabin with U0",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "full = pd.concat([train, test], ignore_index=True)\n_ = full.set_value(full.Cabin.isnull(), 'Cabin', 'U0')",
   "execution_count": 29,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Feature Engineering",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Create a feature, Names, to store the length of words in name.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import re\nnames = full.Name.map(lambda x: len(re.split(' ', x)))\n_ = full.set_value(full.index, 'Names', names)\ndel names",
   "execution_count": 30,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Create a feature, Title.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "title = full.Name.map(lambda x: re.compile(', (.*?)\\.').findall(x)[0])\ntitle[title=='Mme'] = 'Mrs'\ntitle[title.isin(['Ms','Mlle'])] = 'Miss'\ntitle[title.isin(['Don', 'Jonkheer'])] = 'Sir'\ntitle[title.isin(['Dona', 'Lady', 'the Countess'])] = 'Lady'\ntitle[title.isin(['Capt', 'Col', 'Major', 'Dr', 'Officer', 'Rev'])] = 'Officer'\n_ = full.set_value(full.index, 'Title', title)\ndel title",
   "execution_count": 31,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Create a feature, Deck. It may represents the socioeconomic status.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "deck = full[~full.Cabin.isnull()].Cabin.map( lambda x : re.compile(\"([a-zA-Z]+)\").search(x).group())\ndeck = pd.factorize(deck)[0]\n_ = full.set_value(full.index, 'Deck', deck)\ndel deck",
   "execution_count": 32,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Create a feature, Room. It may represents the geo lacation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "checker = re.compile(\"([0-9]+)\")\ndef roomNum(x):\n    nums = checker.search(x)\n    if nums:\n        return int(nums.group())+1\n    else:\n        return 1\nrooms = full.Cabin.map(roomNum)\n_ = full.set_value(full.index, 'Room', rooms)\ndel checker, roomNum\nfull['Room'] = full.Room/full.Room.sum()",
   "execution_count": 33,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Create a feature, Group_num. It may represents the size of family.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "full['Group_num'] = full.Parch + full.SibSp + 1",
   "execution_count": 34,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Create a feature, Group_size. When the size is between 2 and 4, more people are survived.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "full['Group_size'] = pd.Series('M', index=full.index)\n_ = full.set_value(full.Group_num>4, 'Group_size', 'L')\n_ = full.set_value(full.Group_num==1, 'Group_size', 'S')",
   "execution_count": 35,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Normalized the fare.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nfull['NorFare'] = pd.Series(scaler.fit_transform(full.Fare.reshape(-1,1)).reshape(-1), index=full.index)",
   "execution_count": 36,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def setValue(col):\n    _ = train.set_value(train.index, col, full[:891][col].values)\n    _ = test.set_value(test.index, col, full[891:][col].values)\n\nfor col in ['Deck', 'Room', 'Group_size', 'Group_num', 'Names', 'Title']:\n    setValue(col)",
   "execution_count": 37,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Predict Age",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "full.drop(labels=['PassengerId', 'Name', 'Cabin', 'Survived', 'Ticket', 'Fare'], axis=1, inplace=True)\nfull = pd.get_dummies(full, columns=['Embarked', 'Sex', 'Title', 'Group_size'])",
   "execution_count": 38,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.model_selection import train_test_split\nX = full[~full.Age.isnull()].drop('Age', axis=1)\ny = full[~full.Age.isnull()].Age\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)",
   "execution_count": 40,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import make_scorer\n\ndef get_model(estimator, parameters, X_train, y_train, scoring):  \n    model = GridSearchCV(estimator, param_grid=parameters, scoring=scoring)\n    model.fit(X_train, y_train)\n    return model.best_estimator_",
   "execution_count": 44,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import xgboost as xgb\n\nXGB = xgb.XGBRegressor(max_depth=4, seed= 42)\nscoring = make_scorer(mean_absolute_error, greater_is_better=False)\nparameters = {'reg_alpha':np.linspace(0.1,1.0,5), 'reg_lambda': np.linspace(1.0,3.0,5)}\nreg_xgb = get_model(XGB, parameters, X_train, y_train, scoring)\nprint (reg_xgb)",
   "execution_count": 45,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print (\"Mean absolute error of test data: {}\".format(mean_absolute_error(y_test, reg_xgb.predict(X_test))))",
   "execution_count": 47,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "fig = plt.figure(figsize=(15, 6))\nalpha = 0.5\nfull.Age.value_counts().plot(kind='density', color='#FA2379', label='Before', alpha=alpha)\n\npred = reg_xgb.predict(full[full.Age.isnull()].drop('Age', axis=1))\nfull.set_value(full.Age.isnull(), 'Age', pred)\n\nfull.Age.value_counts().plot(kind='density', label='After', alpha=alpha)\nplt.xlabel('Age')\nplt.title(\"What's the distribution of Age after predicting?\" )\nplt.legend(loc='best')\nplt.grid()",
   "execution_count": 48,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "full['NorAge'] = pd.Series(scaler.fit_transform(full.Age.reshape(-1,1)).reshape(-1), index=full.index)\nfull['NorNames'] = pd.Series(scaler.fit_transform(full.Names.reshape(-1,1)).reshape(-1), index=full.index)\nfull['Group_num'] = pd.Series(scaler.fit_transform(full.Group_num.reshape(-1,1)).reshape(-1), index=full.index)",
   "execution_count": 49,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "for col in ['NorAge', 'NorFare', 'NorNames', 'Group_num']:\n    setValue(col)",
   "execution_count": 50,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Encoding sex, feamle: 0 and male: 1",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train.Sex = np.where(train.Sex=='female', 0, 1)\ntest.Sex = np.where(test.Sex=='female', 0, 1)",
   "execution_count": 51,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Convert values of Embarked and Ticket into dummy variables",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train.drop(labels=['PassengerId', 'Name', 'Names', 'Cabin', 'Ticket', 'Age', 'Fare'], axis=1, inplace=True)\ntest.drop(labels=['Name', 'Names', 'Cabin', 'Ticket', 'Age', 'Fare'], axis=1, inplace=True)",
   "execution_count": 52,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train = pd.get_dummies(train, columns=['Embarked', 'Pclass', 'Title', 'Group_size'])\ntest = pd.get_dummies(test, columns=['Embarked', 'Pclass', 'Title', 'Group_size'])\ntest['Title_Sir'] = pd.Series(0, index=test.index)",
   "execution_count": 53,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Build Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.model_selection import learning_curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5), scoring='accuracy'):\n    plt.figure(figsize=(10,6))\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(scoring)\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, scoring=scoring,\n                                                            n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt",
   "execution_count": 54,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import roc_curve, auc\ndef plot_roc_curve(estimator, X, y, title):\n    # Determine the false positive and true positive rates\n    fpr, tpr, _ = roc_curve(y, estimator.predict_proba(X)[:,1])\n\n    # Calculate the AUC\n    roc_auc = auc(fpr, tpr)\n    print ('ROC AUC: %0.2f' % roc_auc)\n\n    # Plot of a ROC curve for a specific class\n    plt.figure(figsize=(10,6))\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve - {}'.format(title))\n    plt.legend(loc=\"lower right\")\n    plt.show()",
   "execution_count": 56,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "X = train.drop(['Survived'], axis=1)\ny = train.Survived\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)",
   "execution_count": 57,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import accuracy_score\nscoring = make_scorer(accuracy_score, greater_is_better=True)",
   "execution_count": 59,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### KNN",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier(weights='uniform')\nparameters = {'n_neighbors':[3,4,5], 'p':[1,2]}\nclf_knn = get_model(KNN, parameters, X_train, y_train, scoring)",
   "execution_count": 60,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print (accuracy_score(y_test, clf_knn.predict(X_test)))\nprint (clf_knn)\nplot_learning_curve(clf_knn, 'KNN', X, y, cv=4);",
   "execution_count": 62,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Random Forest",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(random_state=42, criterion='entropy', min_samples_split=5, oob_score=True)\nparameters = {'n_estimators':[500], 'min_samples_leaf':[12]}\nclf_rfc1 = get_model(rfc, parameters, X_train, y_train, scoring)",
   "execution_count": 63,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print (accuracy_score(y_test, clf_rfc1.predict(X_test)))\nprint (clf_rfc1)\nplot_learning_curve(clf_rfc1, 'Random Forest', X, y, cv=4);",
   "execution_count": 64,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "plt.figure(figsize=(10,6))\nplt.barh(np.arange(X_train.columns.shape[0]), clf_rfc1.feature_importances_, 0.5)\nplt.yticks(np.arange(X_train.columns.shape[0]), X_train.columns)\nplt.grid()\nplt.xticks(np.arange(0,0.2,0.02));",
   "execution_count": 65,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "cols = X_train.columns[clf_rfc1.feature_importances_>=0.016]",
   "execution_count": 66,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "rfc = RandomForestClassifier(random_state=42, criterion='entropy', min_samples_split=5, oob_score=True)\nparameters = {'n_estimators':[500], 'min_samples_leaf':[12]}\nclf_rfc2 = get_model(rfc, parameters, X_train[cols], y_train, scoring)",
   "execution_count": 67,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print (clf_rfc2)\nprint (accuracy_score(y_test, clf_rfc2.predict(X_test[cols])))\nplot_learning_curve(clf_rfc2, 'Random Forest', X[cols], y, cv=4);",
   "execution_count": 68,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Logistic Regression",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.linear_model import LogisticRegression\nlg = LogisticRegression(random_state=42, penalty='l1')\nparameters = {'C':[0.5]}\nclf_lg1 = get_model(lg, parameters, X_train, y_train, scoring)",
   "execution_count": 69,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print (clf_lg1)\nprint (accuracy_score(y_test, clf_lg1.predict(X_test)))\nplot_learning_curve(clf_lg1, 'Logistic Regression', X, y, cv=4);",
   "execution_count": 70,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### SVC",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.svm import SVC\nsvc = SVC(random_state=42, kernel='poly', probability=True)\nparameters = {'C': [35], 'gamma': [0.0055], 'coef0': [0.1],\n              'degree':[2]}\nclf_svc = get_model(svc, parameters, X_train, y_train, scoring)",
   "execution_count": 71,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print (clf_svc)\nprint (accuracy_score(y_test, clf_svc.predict(X_test)))\nplot_learning_curve(clf_svc, 'SVC', X, y, cv=4);",
   "execution_count": 72,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### XGBoost",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import xgboost as XGB\nxgb = XGB.XGBClassifier(seed=42, max_depth=3, objective='binary:logistic', n_estimators=400)\nparameters = {'learning_rate':[0.1],\n              'reg_alpha':[3.0], 'reg_lambda': [4.0]}\nclf_xgb1 = get_model(xgb, parameters, X_train, y_train, scoring)",
   "execution_count": 73,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print (accuracy_score(y_test, clf_xgb1.predict(X_test)))\nprint (clf_xgb1)\nplot_learning_curve(clf_xgb1, 'XGB', X, y, cv=4);",
   "execution_count": 74,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Ensemble",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.ensemble import VotingClassifier\nclf_vc = VotingClassifier(estimators=[('xgb1', clf_xgb1), ('lg1', clf_lg1), ('svc', clf_svc), \n                                      ('rfc1', clf_rfc1),('rfc2', clf_rfc2), ('knn', clf_knn)], \n                          voting='hard', weights=[4,1,1,1,1,2])\nclf_vc = clf_vc.fit(X_train, y_train)",
   "execution_count": 75,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print (accuracy_score(y_test, clf_vc.predict(X_test)))\nprint (clf_vc)\nplot_learning_curve(clf_vc, 'Ensemble', X, y, cv=4);",
   "execution_count": 76,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Make submission",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "PassengerId = test.PassengerId\ntest.drop('PassengerId', axis=1, inplace=True)",
   "execution_count": 77,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def submission(model, fname, X):\n    ans = pd.DataFrame(columns=['PassengerId', 'Survived'])\n    ans.PassengerId = PassengerId\n    ans.Survived = pd.Series(model.predict(X), index=ans.index)\n    ans.to_csv(fname, index=False)",
   "execution_count": 78,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": [],
   "metadata": {}
  }
 ]
}